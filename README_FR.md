<div>
    <h1> <img src="docs/images/logo.png" height=33 align="texttop">OmAgent</h1>
</div>

<p align="center">
  <img src="docs/images/icon.png" width="300"/>
</p>

<p align="center">
    <a>Anglais</a> | <a href="README_ZH.md">ä¸­æ–‡</a> | <a href="README_JP.md">æ—¥æœ¬èª</a>
</p>


## ğŸ—“ï¸ Mises Ã  jour
* 20/09/2024 : Notre article a Ã©tÃ© acceptÃ© par l'EMNLP 2024. Rendez-vous Ã  Miami !ğŸ
* 04/07/2024 : Le projet open-source OmAgent a Ã©tÃ© dÃ©voilÃ©. ğŸ‰
* 24/06/2024Â : [Le document de recherche OmAgent a Ã©tÃ© publiÃ©.](https://arxiv.org/abs/2406.16620)




## ğŸ“–PrÃ©sentation
OmAgent est un systÃ¨me d'agent intelligent multimodal sophistiquÃ©, dÃ©diÃ© Ã  l'exploitation de la puissance des grands modÃ¨les de langage multimodaux et d'autres algorithmes multimodaux pour accomplir des tÃ¢ches intrigantes. Le projet OmAgent englobe un framework d'agent intelligent lÃ©ger, omagent_core, mÃ©ticuleusement conÃ§u pour relever les dÃ©fis multimodaux. Avec ce cadre, nous avons construit un systÃ¨me complexe de comprÃ©hension vidÃ©o longue durÃ©e : OmAgent. Naturellement, vous avez la libertÃ© de lâ€™utiliser pour rÃ©aliser nâ€™importe laquelle de vos idÃ©es innovantes.  
OmAgent comprend trois composants principauxÂ :  
- **Video2RAG** â€‹â€‹: Le concept derriÃ¨re ce composant est de transformer la comprÃ©hension de longues vidÃ©os en une tÃ¢che RAG multimodale. Lâ€™avantage de cette approche est quâ€™elle transcende les limitations imposÃ©es par la durÃ©e de la vidÃ©o ; cependant, l'inconvÃ©nient est qu'un tel prÃ©traitement peut entraÃ®ner une perte de dÃ©tails vidÃ©o substantiels.  
- **DnCLoop**Â : InspirÃ©s par le paradigme algorithmique classique de Divide and Conquer, nous avons conÃ§u une logique de traitement rÃ©cursive de tÃ¢ches gÃ©nÃ©rales. Cette mÃ©thode affine de maniÃ¨re itÃ©rative des problÃ¨mes complexes dans un arbre de tÃ¢ches, transformant finalement les tÃ¢ches complexes en une sÃ©rie de tÃ¢ches plus simples et rÃ©solubles.  
- **Rewinder Tool** : Pour rÃ©soudre le problÃ¨me de perte d'informations dans le processus Video2RAG, nous avons conÃ§u un outil de Â« barre de progression Â» nommÃ© Rewinder qui peut Ãªtre utilisÃ© de maniÃ¨re autonome par les agents. Cela permet aux agents de revoir tous les dÃ©tails de la vidÃ©o, leur permettant ainsi de rechercher les informations nÃ©cessaires.  

<p align="center">
  <img src="docs/images/OmAgent.png" width="700"/>
</p>

Pour plus de dÃ©tails, consultez notre article **[OmAgentÂ : A Multi-modal Agent Framework for Complex Video Understanding with Task Divide-and-Conquer](https://arxiv.org/abs/2406.16620)**

## ğŸ› ï¸ Comment installer
-python >= 3.10
- Installer omagent_core
  ```bash
  cd omagent-core
  pip install -e .
  ```
- Autres exigences
  ```bash
  cd..
  pip install -r exigences.txt
  ```
## ğŸš€ DÃ©marrage rapide

### Traitement gÃ©nÃ©ral des tÃ¢ches
1. CrÃ©ez un fichier de configuration et dÃ©finissez certaines variables nÃ©cessaires
   ```coquille
   cd workflows/gÃ©nÃ©ral && vim config.yaml
   ```

| Nom de la configuration | Utilisation |
   |------------------------------|----------------------- -------------------------------------------------- ---------------|
   | custom_openai_endpoint | Adresse API pour appeler OpenAI GPT ou autre MLLM, formatÂ : ```{custom_openai_endpoint}/chat/completions``` |
   | custom_openai_key | api_key fourni par le fournisseur MLLM |
   | bing_api_key | ClÃ© API de Bing, utilisÃ©e pour la recherche sur le Web |


2. Configurez ```run.py```
    ```python
    def run_agent (tÃ¢che)Â :
        logging.init_logger("omagent", "omagent", level="INFO")
        registre.import_module(project_root=Path(__file__).parent, custom=["./engine"])
        bot_builder = Builder.from_file("workflows/general") # RÃ©pertoire de configuration du workflow de traitement des tÃ¢ches gÃ©nÃ©rales
        entrÃ©e = DnCInterface(bot_id="1", task=AgentTask(id=0, task=task))
    
        bot_builder.run_bot(entrÃ©e)
        retourner input.last_output
    
    
    si __name__ == "__main__":
        run_agent("Votre requÃªte") # Entrez votre requÃªte
    ```
3. DÃ©marrez OmAgent en exÃ©cutant ```python run.py```.

### TÃ¢che de comprÃ©hension de la vidÃ©o
#### PrÃ©paration de l'environnement
- **```Facultatif```** OmAgent utilise Milvus Lite comme base de donnÃ©es vectorielle pour stocker les donnÃ©es vectorielles par dÃ©faut. Si vous souhaitez utiliser le service Milvus complet, vous pouvez le dÃ©ployer [base de donnÃ©es vectorielles milvus](https://milvus.io/docs/install_standalone-docker.md) Ã  l'aide de Docker. La base de donnÃ©es de vecteurs est utilisÃ©e pour stocker des vecteurs de caractÃ©ristiques vidÃ©o et rÃ©cupÃ©rer des vecteurs pertinents sur la base de requÃªtes afin de rÃ©duire le calcul MLLM. Docker non installÃ© ? Reportez-vous au [guide d'installation de Docker](https://docs.docker.com/get-docker/).
    ```coquille
       # TÃ©lÃ©charger le script de dÃ©marrage de Milvus
       curl -sfL https://raw.githubusercontent.com/milvus-io/milvus/master/scripts/standalone_embed.sh -o standalone_embed.sh
       # DÃ©marrez Milvus en mode autonome
       bash standalone_embed.sh dÃ©marrer
    ```
    Remplissez les informations de configuration pertinentes aprÃ¨s le dÃ©ploiement ```workflows/video_understanding/config.yml```  
    
- **```Facultatif```** Configurez l'algorithme de reconnaissance faciale. Lâ€™algorithme de reconnaissance faciale peut Ãªtre appelÃ© comme outil par lâ€™agent, mais il est facultatif. Vous pouvez dÃ©sactiver cette fonctionnalitÃ© en modifiant le fichier de configuration ```workflows/video_understanding/tools/video_tools.json``` et en supprimant la section FaceRecognition. La base de donnÃ©es de reconnaissance faciale par dÃ©faut est stockÃ©e dans le rÃ©pertoire ```data/face_db```, avec diffÃ©rents dossiers correspondant Ã  diffÃ©rents individus.
- **```Facultatif```** Service de dÃ©tection de vocabulaire ouvert (ovd), utilisÃ© pour amÃ©liorer la capacitÃ© d'OmAgent Ã  reconnaÃ®tre divers objets. Les outils ovd dÃ©pendent de ce service, mais il est facultatif. Vous pouvez dÃ©sactiver les outils ovd en suivant ces Ã©tapes. Supprimez les Ã©lÃ©ments suivants de ```workflows/video_understanding/tools/video_tools.json```
```json 
       {
            "name": "DÃ©tection d'Objet",
            "ovd_endpoint": "$<ovd_endpoint::http://host_ip:8000/inf_predict>",
            "model_id": "$<ovd_model_id::OmDet-Turbo_tiny_SWIN_T>"
       }
    ```
  
  Si vous utilisez des outils ovd, nous utilisons [OmDet](https://github.com/om-ai-lab/OmDet/tree/main) pour la dÃ©monstration.
  1. Installez OmDet et son environnement conformÃ©ment au [Guide d'installation d'OmDet](https://github.com/om-ai-lab/OmDet/blob/main/install.md).
  2. Installez les exigences pour transformer l'infÃ©rence OmDet en appels API
     ```texte
      pip installer pydantic fastapi uvicorn
     ```
  3. CrÃ©ez un fichier ```wsgi.py``` pour exposer l'infÃ©rence OmDet en tant qu'API
     ```coquille
      cd OmDet && vim wsgi.py
     ```
     Copiez le [code API d'infÃ©rence OmDet] (docs/ovd_api_doc.md) dans wsgi.py
  4. DÃ©marrez l'API d'infÃ©rence OmDet, le port par dÃ©faut est 8000
     ```coquille
     python wsgi.py
     ```
- TÃ©lÃ©chargez des vidÃ©os intÃ©ressantes

#### PrÃ©paration en cours d'exÃ©cution
1. CrÃ©ez un fichier de configuration et dÃ©finissez certaines variables d'environnement nÃ©cessaires
   ```coquille
   cd workflows/video_understanding && vim config.yaml
   ```
2. Configurez les adresses API et les clÃ©s API pour MLLM et les outils.

   | Nom de la configuration | Utilisation |
   |-------------------------------|---------------------- -------------------------------------------------- -----------------|
   | custom_openai_endpoint | Adresse API pour appeler OpenAI GPT ou autre MLLM, formatÂ : ```{custom_openai_endpoint}/chat/completions``` |
   | custom_openai_key | api_key fourni par le fournisseur d'API respectif |
   | bing_api_key | ClÃ© API de Bing, utilisÃ©e pour la recherche sur le Web |
   | ovd_endpoint | Adresse API de l'outil ovd. Si vous utilisez OmDet, l'adresse doit Ãªtre ```http://host:8000/inf_predict``` |
   | ovd_model_id | ID de modÃ¨le utilisÃ© par l'outil ovd. Si vous utilisez OmDet, l'ID du modÃ¨le doit Ãªtre ```OmDet-Turbo_tiny_SWIN_T``` |

2. Configurez ```run.py```
    ```python
    def run_agent (tÃ¢che)Â :
        logging.init_logger("omagent", "omagent", level="INFO")
        registre.import_module(project_root=Path(__file__).parent, custom=["./engine"])
        bot_builder = Builder.from_file("workflows/video_understanding") # RÃ©pertoire de configuration du workflow des tÃ¢ches de comprÃ©hension vidÃ©o
        entrÃ©e = DnCInterface(bot_id="1", task=AgentTask(id=0, task=task))
    
        bot_builder.run_bot(entrÃ©e)
        retourner input.last_output
    
    
    si __name__ == "__main__":
        run_agent("") # Vous serez invitÃ© Ã  saisir la requÃªte dans la console
    ```
3. DÃ©marrez OmAgent en exÃ©cutant ```python run.py```. Entrez le chemin de la vidÃ©o que vous souhaitez traiter, attendez un moment, puis entrez votre requÃªte et OmAgent rÃ©pondra en fonction de la requÃªte.


## ğŸ”— Å’uvres liÃ©es
Si vous Ãªtes intriguÃ© par les algorithmes multimodaux, les grands modÃ¨les de langage et les technologies d'agents, nous vous invitons Ã  approfondir nos recherchesÂ :  
ğŸ”† [Comment Ã©valuer la gÃ©nÃ©ralisation de la dÃ©tection ? Une rÃ©fÃ©rence pour une dÃ©tection complÃ¨te du vocabulaire ouvert](https://arxiv.org/abs/2308.13177)(AAAI24)   
ğŸ  [DÃ©pÃ´t Github](https://github.com/om-ai-lab/OVDEval/tree/main)

ğŸ”† [OmDetÂ : prÃ©-formation multi-ensembles de donnÃ©es en langage de vision Ã  grande Ã©chelle avec rÃ©seau de dÃ©tection multimodal](https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/cvi2.12268)(IET Computer Vision)  
ğŸ  [DÃ©pÃ´t Github](https://github.com/om-ai-lab/OmDet)

## â­ï¸Citation

Si vous trouvez notre rÃ©fÃ©rentiel utile, veuillez citer notre articleÂ :  
```angulaire2
@article{zhang2024omagent,
  title={OmAgentÂ :Â un cadre d'agent multimodal pour la comprÃ©hension de vidÃ©os complexes avec la division des tÃ¢ches pour rÃ©gner},
  author={Zhang, Lu et Zhao, Tiancheng et Ying, Heting et Ma, Yibo et Lee, Kyusong},
  journal={prÃ©impression arXiv arXiv:2406.16620},
  annÃ©e={2024}
}
```
